# L3: Loss Functions and Optimization

回忆一下上一节课的线性分类函数 $f(x,W)$，我们现在关注这个 $W$。对于训练集中的每个图片的每个像素，在矩阵 $W$ 里都存在一个对应的元素告诉我们这个像素会对分类造成多少影响。也就是说 $W$ 里的每一行都对应了一类图片和一些权重，如果我们把 $W$ 矩阵的某一行恢复回去，我们就可以更加直观的看到每一行对应的是什么东西。

另一种解释是，线性分类的每一行实际上是学习像素在高维空间的一个线性决策边界。高维空间对应了像素密度值。那么怎么选择一个好的 $W$ 呢？

我们需要一个度量任意某个 $W$ 好坏的方法，也就是我们提出的**损失函数(loss function)**。

## 损失函数(loss function)

我们定义对于数据集 $\{(x_i,y_i)\}^N_{i=1}$，其中 $x_i$ 代表图像,$y_i$ 代表标签。我们定义数据集上的 loss 为：
$$
L = \frac{1}{N} \sum_i L_i(f(x_i,W),y_i)
$$
$L$ 的意义在于给了我们一种方式定量考察每个 $W$ 的优秀程度，那么我们就可以采用其他方法来寻找最好的 $W$.

## 多分类 SVM

正常的 SVM 是进行二分类的利器，我们通过推广一下 SVM 可以让他执行多分类。这里的 loss 函数我们可以这么定义，对于每个个体 $L_i$，我们对于除了 $y_i$ 以外的所有分类做加和，即对于所有错误的分类做加和。如果正确分类的分数比错误分类的分数高出某个bound（此处设为 1），那么此时的 loss 就是 0，否则就是错误分数和正确分数做差。

$$
L_i = \sum_{j\not= y_i} \max(0,S_j - S_{y_i} + 1)
$$

这种 max 类型的 loss 函数也被称为折页损失函数(Hinge Loss)关于为什么这个 bound 取 1？因为实际上我们不在乎 loss 的具体大小，我们在乎的都是相对差值，所以取多少并不重要，那我们就取 1 咯。

考虑一个问题，如果我们找到了一个 $W$ 使得 $L = 0$，这个 $W$ 是否唯一？显然不是，因为 $2W$ 仍然可以令 $L = 0$。

在实际应用中，由于过拟合问题的存在，我们会更希望 $W$ 这个东西更加规律一点，这样才不会对于测试集也跑出一些我们不想见到的结果，同时也是为了让模型更加简单。所以我们实际上的损失函数 $L$ 应当加上正则化的刻画：

$$
L(W) = \frac{1}{N} \sum\limits_{i = 1}^N L_i(f(x_i,W),y_i) + \lambda R(W)
$$

这个 $R(W)$ 是我们用来刻画 $W$ 是否“简单”的函数.在大部分应用中，我们使用 L2 正则化，也就是
$$
R(W) = \sum_k \sum_l W_{k,l}^2
$$

对 $W$ 进行惩罚可以使得我们的 $W$ 往低阶多项式演进，而我们上面讨论的所有都是对于多分类 SVM 考虑的。

## 多项逻辑回归(Softmax loss)

同样是深度学习里很广泛的一个函数。刚才的 SVM 里我们没有过多解释分数的含义，但是在 softmax loss 里，我们定义分数=没有归一化的每一类的对数概率。为了让他们都是正数，我们有
$$
P(Y=K|X=x_i) = \frac{e^s\times k}{\sum_j e^s\times j}
$$

这个函数就被称为 **softmax 损失函数**，我们可以定义 
$$
L_i = -\log P(Y = y_i | X = x_i)
$$
对于正确的类别，概率应该尽量高并趋近于 1。所以可以用 softmax 函数衡量预测概率与真实标签的差异，所以最后就是：

$$
L_i = -\log \frac{e^s\times y_i}{\sum_j e^s\times j} = \log \frac{\sum_j e^s\times j}{e^s\times y_i}
$$

## 优化

算法竞赛选手醒了，这里就是爬山/模拟退火算法魅力时刻!我们知道随机搜索的效率很差，思考一下发现，我们希望让一个多项式函数达到最小值，由此考虑一个函数的梯度，我们会尽量往梯度小的地方走。而这就是**梯度下降**.