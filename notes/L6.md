# L6:训练神经网络 

## 激活函数

我们之前用到的 ReLU 就是一种非线性的激活函数，我们将介绍很多种激活函数：

- Sigmoid 函数

$\sigma(x) = \frac{1}{1 + e ^{-x}}$ 它可以把所有值放缩到 $[0,1]$ 范围里，然后当越接近负无穷的时候趋近于 0，正无穷趋近于 1.

但是问题在于，sigmoid 函数当 $|x|$ 过大的时候，会让梯度消失。就是说只有在 0 附近的时候有比较恰当的梯度值。并且计算 `exp(x)` 一般来说是一个非常耗时间的东西。

- $tanh(x)$

和 sigmoid 差不多，但是他以 0 作为中心，不过仍然会发生梯度消失的现象。

- ReLU

形式：$f(x) = \max(0,x)$ 

ReLU 在正数（对应一半的空间）不会产生梯度消失的问题，并且计算非常快，只需要取 $\max$ 即可。并且它非常符合我们的生物神经元。

缺点：他不是以 0 为中心的，并且当 $x < 0$ 的时候梯度消失了。这种现象称为 dead ReLU,他们永远不会被激活，也就是永远无法更新。

- Leaky ReLU

$f(x) = \max(0.01x,x)$

好处在于不会在 $x<0$ 的部分挂掉，同时仍然非常高效。

- ELU

- Maxout

$f(x) = \max(w_1^T x + b_1,w_2^T x + b_2)$

线性计算，不会

## 数据处理

### 1.预处理数据

把数据转化为以 0 为中心的所有数据：`X -= np.mean(X,axis = 0)`  
把数据归一化：`X /= np.std(X,axis = 0)`

注意我们如果对训练集做了这样的归一化，对于验证集也要做同样的工作。我们减去所有均值图像这组的值（类似 AlexNet）,也有可能减去单通道的均值（类似 VGGNet）

## 权重初始化

考虑如果我们最开始[L5.md](L5.md)令所有 $W = 0$，那么所有的神经元影响反馈都是一样的。所以一般来说我们会令 W 最开始先是某个初始随机值，也就是

`W = 0.01 * np.random.randn(D,H)`

这里其实就是从标准高斯取样中抽选，但是这有个问题是对于深层次的网络可能会寄，因为在多层的迭代之后，所有 W 的值会收敛到 0.
但是当权重太大的时候，网络又会两极化。

所以有人提出了 Xavier initialization，也就是 `W = np.random.randn(fan_in,fan_out) / np.sqrt(fan.in)`

## 批量归一化

提升深度网络训练效果的一种方法是采用更先进的优化算法（如带动量的SGD、RMSProp或Adam）。另一种策略是调整网络架构本身，在2015年提出的批量归一化正是这类思路的体现。

理解批量归一化的目标前，需明确一个前提：当输入数据的特征互不相关且符合零均值、单位方差时，机器学习方法通常表现更好。虽然我们可以在数据输入网络前进行预处理来实现这一点，但随着数据在网络深层传递时，各层输出的激活值会逐渐偏离这种理想分布。更严重的是，网络权重更新会导致各层特征的分布持续漂移。

应用于小批量算法的 BN 如下所示：

![](https://picx.zhimg.com/v2-10d551f90e66d1a790a2c3a7d03afdf3_1440w.jpg)

其中，BN只增加了一个 $\gamma$ 和一个 $\beta$ 即完成了BN的操作，且它们的计算也并不复杂，这保持了网络的表示能力。
