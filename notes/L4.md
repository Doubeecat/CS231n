# L4:Backpropagation and Neural Networks

引入计算图的概念：计算图的本质是一个 DAG，其中每个节点代表了运算符或者变量或者函数，通过这样的方式我们可以用计算图表示任何一个函数。建出图之后，我们可以通过反向传播（也就是在反图上递推）然后得出每个变量的梯度，换句话说，我们可以通过计算图对任意复杂函数进行求导。

反向递推如何实现？本质上就是对反图进行拓扑排序，根据链式法则，求导是满足递推性质的。

反向传播的连续节点可以任意组合,只要对这个整体求导即可.

$\max$ 节点的后向传播与前向传播类似,它只将梯度传递给最大值对应的节点，所以相当于只把 1 传给对应的方向罢了。

沿着梯度方向前进一个步长,就更新了权重.

利用线性分类函数的各层叠加，我们可以得到多层的**神经网络**。

## 神经网络

举个例子，2 层的神经网络是 $f = W_2 \max(0,W_1 x)$，如果我们需要生成一个多层的复杂神经网络，我们就可以通过线性层与线性层的叠加得到一个非线性函数。具体而言，相当于把我们作业中的分类器改造了一下，从一个 3072→10 的分类器变成了一个 3072→100→10 的分类器，也就允许了我们支持识别更多东西，因为中间这个层给我们提供了更多特征信息。