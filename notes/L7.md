# L7:训练神经网络（下）

## momentum + SGD

核心思想就是，SGD 对于某些函数收敛的特别慢，这类函数的梯度方向并不与最小值为一条线，导致我们的收敛曲线和一个折线一样。并且容易在鞍点掉进去。

所以我们在 SGD 中引入一个动量项，也就是 momentum + SGD。原本的 SGD 是 $x_{t+1} = x_t - \alpha \Delta f(x_t)$，引入动量之后变为 
$$
v_{t + 1} = \rho v_t  + \Delta f(x_t) \\
x_{t + 1} = x _t - \alpha v_{t + 1}
$$

有时候我们会看到一个比较小的改变，也就是 Nesterov 动量，它帮助我们克服噪声

$$
v_{t + 1} = \rho v_t  - \alpha \Delta f(x_t + \rho x_t) \\
x_{t + 1} = x _t + v_{t + 1}
$$

更多优化的方法：AdaGrad,RMSProp，核心思想都是先求梯度平方的估计值，然后减去梯度平方的实际值。

## Adam

综合以上两部分，我们提出 Adam 算法，也就是把 Momentum 和 AdaGrad 结合在一起。

计算第一部分动量，我们使用 momentum 算法，第二部分动量使用 AdaGrad 算法：

```py
first_moment,second_moment = 0,0
while True:
    dx = compute_gradiant(x)
    first_moment = beta1 * first_moment + (1 - beta1) * dx
    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx
    x -= lr * first_moment / (np.sqrt(second_moment) + eps)
```

对梯度的一阶矩估计（均值）和二阶矩估计（未中心化的方差）进行综合运用,为了弥补最开始我们可能得到很大步长错过太多东西，我们也会引入误差修正：

```py
first_unbias = first_moment / (1 - beta1 ** t)
second_unbias = second_moment / (1 - beta2 ** t)
```

Adam 采用 `beta1 = 0.9,beta2 = 0.99,learning_rate = 1e-3,5e-4` 的话是最好的起始点（对大部分模型）

## Dropout 正则化

为了避免过拟合，我们经常加上一个正则项。我们之前提到过的 L2 正则化就是如此，有时候我们会在神经网络中用到其他策略，比如今天要用到的 **Dropout 正则化**。我们随机选择一些神经元然后归0，

## 迁移学习

