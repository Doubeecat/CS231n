# Lecture 2: Image Classification pipeline

图像识别是 CV 中最核心的任务之一。对于计算机而言，我们的一个图片对他来说就是一个 800 * 600 * 3 (取决于精度和尺寸) 的矩阵。对于孤立的图片而言，我们找到他的逻辑(或者说模式)是相对简单的。

但是图片实际上有很多，我们还需要考虑不同角度/光照/姿态/背景...下的同一个物品，比如一只猫。显然我们没有办法对这个东西进行 hard-code，我们需要使用更加强力的方法：**数据驱动方法**，总体而言数据驱动方法就是两个函数：
```py
def train(images,labels):
    # Machine learning!!
    return model
# 通过机器学习来学习所有图片和标签的关系
def predict(model,test_images):
    # prediction
    return test_labels
# 预测最接近图片的标签
```

## k-最近邻分类器

我们会用到的一个比较出名的数据集：CIFAR10，有 50000 张图片作为训练集和 10000 张图片作为测试集。

接下来的问题是，如果我们给出两个图片，如何比较他们的相似程度？我们可以使用 L1 距离（即曼哈顿距离）去刻画两张图片的相似度，也就是建造一个**最近邻分类器**。

$$
d_1(I_1,I_2) = \limits\sum_p |I_1^p - I_2^p|
$$

这个公式很简单，我们只是朴素的把图片矩阵中每个位置上的像素代表值作差取绝对值后求和。这很简单，不过也给了我们一种方法如何去比较。这种方法训练的时间复杂度是 $O(1)$，但是预测的复杂度就达到了 $O(n)$，这并不是我们愿意看到的。在实际应用中我们肯定希望预测跑的尽量快。

不过这个算法其实是有问题的，很多时候我们会把一些点给误判，那么如何改进？联想到多数投票，我们实际上可以让他寻找最近的 $k$ 个训练集中的图片，然后让这些标签进行多数投票。这就是我们的 **k-最近邻分类器(KNN,K-Nearest Leighbors)**，通过恰当设计 $k$ 的大小，就可以过滤掉噪声。把 k 设计的相对大一些就可以过滤掉极端数据，也就是说没有获得 K 票的奇奇怪怪的图片，这类图片我们可以把他们单独分出来一类。

而对于距离选择，更加常见的则是 L2 距离（即欧几里得距离），也就是:

$$
d_2(I_1,I_2) = \sqrt{\sum_p(I_1^p - I_2^p) ^ 2}
$$

选择距离的度量会对我们不同的空间造成不同的影响，也就是说，我们从数学底层上对图像空间进行了不同的假设。区别在于，L1 距离对于我们选择的坐标系来说，会根据坐标系不同而造成差异；L2 距离则不会因为坐标轴的差别而发生变化。选择哪种距离取决于我们关注的问题本身。通过选择不同的距离，我们可以不仅局限于在矩阵、图片中分类，我们甚至可以对于文本进行分类！我们只需要定义一个新的 $d_n$ 代表说两个单词/句子之间的距离即可。可以在[这里](http://vision.stanford.edu/teaching/cs231n-demos/knn/)看到一个有既视感的 demo。

关于 k 和距离的度量这种东西，我们称为**超参数(hyperparameters)**，也就是不能通过数据本身学到的东西。而这取决于具体问题，并且很多时候我们试出最好的那个点（也就是熟知的调参^^）

### 如何选择超参数？

Idea 1:只选择在训练集中跑的最好的那些数据

这个实际上是有问题的，因为你可能不知不觉你就纯拟合了训练集而已，比如高考前你猛刷模拟题觉得自己无敌了，结果写真题就寄了。

Idea 2:把训练集划分成两部分，通过用前一部分来检验后一部分。

不应该这样，因为这样实际上没法评估在新数据上的效果。说白了就是根据答案写过程。

Idea 3:把训练集划分为三部分：训练集，验证集，测试集。

正确√

Idea 4:交叉验证，我们保留测试集，把训练数据划分成很多分，轮流把每部分当作训练集。

对于小数据集上很实用，但是不要常用。