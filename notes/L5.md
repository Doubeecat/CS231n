# L5:卷积神经网络

卷积神经网络和神经网络的想法基本一样，但是我们训练的是卷积层，更可以保证特征的保留。整个网络仍表示为一个可微分的评分函数：从一端输入原始图像像素，到另一端输出类别得分。最后一层（全连接层）仍使用SVM/Softmax等损失函数，此前为常规神经网络开发的所有训练技巧仍适用。

## 原理

此前的全连接层类似于把 32x32x3 的图像信息拍扁成 3072x1 的矩阵，然后再通过中间的 $W_x$ 层去得到最后的 10 个输出。而卷积层的特点在于，保留了 32x32x3 的整体结构，然后通过一些比较小(比如 5x5x3) 的卷积核 $w$，通过在整个空间里滑动算出具体细节，也就是从 $T$ 中选取一个 5x5 的子层数，然后用：$T\times w^{T} + b$ 得到每个位置的值，具体计算时候我们可能还是运用类似转置的东西来进行，即虽然看着是一个卷积核，但是实际上我们还是用向量的计算来进行。

根据本质不同的卷积核个数，我们可以得到若干层卷积层。前面的卷积层我们学习到的可能是一些低阶特征，比如说边缘特征。中间层我们可以得到更加复杂的特征，比如边角和斑点……

考虑一下层数大小和卷积的关系，记原矩阵是 $N\times N$的，卷积核是 $F\times F$ 的，那么最终的输出大小就是 $(N-F) / 步幅 + 1$,同时为了保证我们输出的符合方形卷积，我们尽量在x,y轴用一样的steps。

总结一下，卷积层接受一个大小为 $W_1 \times H_1 \times D_1$ 的卷积，并且需要以下超参数：
- 卷积核个数 $K$（一般是 2 的幂次）
- 卷积核空间大小 $F$
- 步幅 $S$
- 零边界 $P$

接下来引入一个东西叫池化层，池化层所做的就是让整个图像的信息变少，这样方便我们控制参数的数量。本质上是在 xy 层上做一个 downsampling。

我们最常用到的东西叫最大池化法(maxpooling)，也就是池化层大小等于卷积层大小，我们和卷积核做类似的工作，不过不是数量积而是取 max，并且我们可以通过设置步长，让池化层每一个元素对应的小元素不重叠。选最大值的含义在于，我们需要取“最有代表性的元素”来进行。

池化层才能够接受一个大小为 $W_1 \times H_1 \times D_1$ 的卷积，并且需要以下超参数：
- 卷积核空间大小 $F$
- 步幅 $S$
生成的卷积层大小：$W_1 \times H_1 \times D_1$:
- $W_2 = (W_1 - F) / S + 1$
- $H_2 = (H_1 - F) / S + 1$
- $D_2 = D_1$

经典设定：$F=2,S = 2 / F = 3,S = 2$

最后，经过若干卷积层和池化层之后我们经历一个全连接层，得到每个类别的得分。